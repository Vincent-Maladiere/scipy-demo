{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2a96d6-0595-4be4-b536-b4233a5de313",
   "metadata": {},
   "source": [
    "![euroscipy2022-logo](../asset/euroscipy2022-logo.png)\n",
    "\n",
    "# Optimization with Scipy\n",
    "\n",
    "*Reading time ~45 min*\n",
    "\n",
    "- Authored by: [Vincent Maladiere](https://github.com/Vincent-Maladiere)\n",
    "- Credits: [Scipy turorial](https://scipy-lectures.org/advanced/mathematical_optimization/auto_examples/plot_noisy.html#sphx-glr-advanced-mathematical-optimization-auto-examples-plot-noisy-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04c7ac-cebc-41fb-a497-e1199b64204f",
   "metadata": {},
   "source": [
    "## Problem definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d656f2-9928-45ea-98fd-6a8771fbe248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb50783-9f03-4253-b81b-ae42cc6a7014",
   "metadata": {},
   "source": [
    "### Convexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6228b605-a85d-4934-a1ef-dcdb1a7da25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 2)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 4))\n",
    "\n",
    "# A convex function\n",
    "axes[0].plot(x, x**2, linewidth=2)\n",
    "axes[0].text(-.7, -.6**2, '$f$', size=20)\n",
    "\n",
    "# The tangent in one point\n",
    "axes[0].plot(x, 2*x - 1)\n",
    "axes[0].plot(1, 1, 'k+')\n",
    "axes[0].text(.3, -.75, \"Tangent to $f$\", size=15)\n",
    "axes[0].text(1, 1 - .5, 'C', size=15)\n",
    "\n",
    "# Convexity as barycenter\n",
    "axes[0].plot([.35, 1.85], [.35**2, 1.85**2])\n",
    "axes[0].plot([.35, 1.85], [.35**2, 1.85**2], 'k+')\n",
    "axes[0].text(.35 - .2, .35**2 + .1, 'A', size=15)\n",
    "axes[0].text(1.85 - .2, 1.85**2, 'B', size=15)\n",
    "axes[0].set_title(\"A convex function\")\n",
    "axes[0].set_ylim(ymin=-1)\n",
    "axes[0].axis('off')\n",
    "\n",
    "# A non convex function\n",
    "axes[1].plot(x, x**2 + np.exp(-5*(x - .5)**2), linewidth=2)\n",
    "axes[1].text(-.7, -.6**2, '$f$', size=20)\n",
    "axes[1].set_title(\"A non-convex function\")\n",
    "axes[1].set_ylim(ymin=-1)\n",
    "axes[1].axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaa95d6-668d-4514-9e48-d37eb8d36b10",
   "metadata": {},
   "source": [
    "A convex function:\n",
    "- f is above all its tangents.\n",
    "- equivalently, for two point A, B, f(C) lies below the segment [f(A), f(B])], if A < C < B\n",
    "\n",
    "Finding the global minimum of a convex functions is easy. Finding it for non-convex function is harder.\n",
    "\n",
    "For convex functions, we can show that if we find a local minimum is also the global minimum. In that sense, the minimum is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a87ba0-216a-4cbb-bf76-bc8d55be84e4",
   "metadata": {},
   "source": [
    "### Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76034d42-47c9-4d49-aea2-05f5d68441a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1.5, 1.5, 101)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 4))\n",
    "\n",
    "# A smooth function\n",
    "axes[0].plot(x, np.sqrt(.2 + x**2), linewidth=2)\n",
    "axes[0].text(-1, 0, '$f$', size=20)\n",
    "axes[0].set_ylim(ymin=-.9)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title(\"Smooth function\")\n",
    "\n",
    "# A non-smooth function\n",
    "axes[1].plot(x, np.abs(x), linewidth=2)\n",
    "axes[1].text(-1, 0, '$f$', size=20)\n",
    "axes[1].set_ylim(ymin=-0.9)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title(\"Non smooth function\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad492d3-2b50-4117-bc1b-cf3b953c7b20",
   "metadata": {},
   "source": [
    "The gradient of a smooth function is defined everywhere, and is a continuous function. \n",
    "\n",
    "Therefore optimizing smooth functions is easier (true in the context of black-box optimization, otherwise Linear Programming is an example of methods which deal very efficiently with piece-wise linear functions)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316368e-b603-4b6a-9e04-e24eea52668f",
   "metadata": {},
   "source": [
    "### Noisy optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d203b7-b846-4d7d-b3f0-5cc150206d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "x = np.linspace(-5, 5, 101)\n",
    "x_ = np.linspace(-5, 5, 31)\n",
    "\n",
    "def f(x):\n",
    "    return -np.exp(-x**2)\n",
    "\n",
    "# A smooth function\n",
    "plt.figure(1, figsize=(3, 2.5))\n",
    "\n",
    "plt.plot(x_, f(x_) + .2*np.random.normal(size=31), linewidth=2, label=\"signal + noise\")\n",
    "plt.plot(x, f(x), linewidth=2, label=\"signal\")\n",
    "\n",
    "plt.ylim(ymin=-1.3)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3b833-f42b-4e0c-a556-5a5416bbab0d",
   "metadata": {},
   "source": [
    "Many optimization methods rely on gradients of the objective function. If the gradient function is not given, they are computed numerically, which induces errors. In such situation, even if the objective function is not noisy, a gradient-based optimization may be a noisy optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8b9bd3-887a-46cf-af63-818e134356a5",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e59a7f-3b29-4d73-9d22-705bb24b7b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.mgrid[-2.9:5.8:.05, -2.5:5:.05]\n",
    "x = x.T\n",
    "y = y.T\n",
    "\n",
    "for i in (1, 2):\n",
    "    # Create 2 figure: only the second one will have the optimization\n",
    "    # path\n",
    "    plt.figure(i, figsize=(3, 2.5))\n",
    "    plt.clf()\n",
    "    plt.axes([0, 0, 1, 1])\n",
    "\n",
    "    contours = plt.contour(np.sqrt((x - 3)**2 + (y - 2)**2),\n",
    "                        extent=[-3, 6, -2.5, 5],\n",
    "                        cmap=plt.cm.gnuplot)\n",
    "    plt.clabel(contours,\n",
    "            inline=1,\n",
    "            fmt='%1.1f',\n",
    "            fontsize=14)\n",
    "    plt.plot([-1.5, -1.5,  1.5,  1.5, -1.5],\n",
    "            [-1.5,  1.5,  1.5, -1.5, -1.5], 'k', linewidth=2)\n",
    "    plt.fill_between([ -1.5,  1.5],\n",
    "                    [ -1.5, -1.5],\n",
    "                    [  1.5,  1.5],\n",
    "                    color='.8')\n",
    "    plt.axvline(0, color='k')\n",
    "    plt.axhline(0, color='k')\n",
    "\n",
    "    plt.text(-.9, 4.4, '$x_2$', size=20)\n",
    "    plt.text(5.6, -.6, '$x_1$', size=20)\n",
    "    plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "\n",
    "# And now plot the optimization path\n",
    "accumulator = list()\n",
    "\n",
    "def f(x):\n",
    "    # Store the list of function calls\n",
    "    accumulator.append(x)\n",
    "    return np.sqrt((x[0] - 3)**2 + (x[1] - 2)**2)\n",
    "\n",
    "\n",
    "# We don't use the gradient, as with the gradient, L-BFGS is too fast,\n",
    "# and finds the optimum without showing us a pretty path\n",
    "def f_prime(x):\n",
    "    r = np.sqrt((x[0] - 3)**2 + (x[0] - 2)**2)\n",
    "    return np.array(((x[0] - 3)/r, (x[0] - 2)/r))\n",
    "\n",
    "optimize.minimize(f, np.array([0, 0]), method=\"L-BFGS-B\",\n",
    "                     bounds=((-1.5, 1.5), (-1.5, 1.5)))\n",
    "\n",
    "accumulated = np.array(accumulator)\n",
    "plt.plot(accumulated[:, 0], accumulated[:, 1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45950a1d-9242-47f8-8124-774af6088a74",
   "metadata": {},
   "source": [
    "$-1 < x1 < 1 \\\\ -1 < x2 < 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033f5bfa-639c-4362-a5d5-fe3b2c848294",
   "metadata": {},
   "source": [
    "## Getting started: 1D optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd09a4e-13b4-4149-bc44-525c8a8b9f5d",
   "metadata": {},
   "source": [
    "### Brent's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2156f-f0eb-49f1-91c4-7cdc2606dd64",
   "metadata": {},
   "source": [
    "Let’s get started by finding the minimum of the scalar function $f(x)=\\exp[(x-0.7)^2]$. \n",
    "\n",
    "`scipy.optimize.minimize_scalar()`\n",
    " uses [Brent’s method](https://en.wikipedia.org/wiki/Brent%27s_method) to find the minimum of a function. \n",
    " \n",
    " Brent's method leverages both the secant method and bisection to find the root of a function $f$. It add few tweaks to the Dekker's methods below:\n",
    " - $b_k$ is the current iterate (aka \"best guess\" for the root of $f$)\n",
    " - $a_k$ is the \"contrapoint\", such that $f(b_k)$ and $f(a_k)$ have opposite signs, so the interval $[a_k, b_k]$ contains the solution.\n",
    " We compute 2 values:\n",
    " 1. secant method: $s = b_k - \\frac{b_k - b_{k-1}}{f(b_k) - f(b_{k-1})}f(b_k)$\n",
    " 2. bisection: $m = \\frac{a_k+b_k}{2}$\n",
    " \n",
    "$b_{k+1} = \\begin{cases} s & \\mathrm{if}\\; s\\; \\in [b_k, m] \\\\ m & \\mathrm{otherwise}\\end{cases}$\n",
    "\n",
    "$a_{k+1} = \\begin{cases} a_k & \\mathrm{if}\\; f(a_k)\\;\\mathrm{and}\\;f(b_k)\\;\\mathrm{have\\;opposite\\;signs} \\\\ b_k & \\mathrm{otherwise} \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ee455-96f2-43cf-9b1c-7dd1b46633bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return -np.exp(-(x-0.7)**2)\n",
    "\n",
    "result = optimize.minimize_scalar(f)\n",
    "result.success, result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c595290e-990d-4097-a266-8a034f39b25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 3, 100)\n",
    "x_0 = np.exp(-1)\n",
    "\n",
    "def f(x):\n",
    "    return (x - x_0)**2 + epsilon*np.exp(-5*(x - .5 - x_0)**2)\n",
    "\n",
    "for epsilon in (0, 1):\n",
    "    plt.figure(figsize=(3, 2.5))\n",
    "    plt.axes([0, 0, 1, 1])\n",
    "\n",
    "    # A convex function\n",
    "    plt.plot(x, f(x), linewidth=2)\n",
    "\n",
    "    # Apply brent method. To have access to the iteration, do this in an\n",
    "    # artificial way: allow the algorithm to iter only once\n",
    "    all_x = list()\n",
    "    all_y = list()\n",
    "    for iter in range(30):\n",
    "        result = optimize.minimize_scalar(f, bracket=(-5, 2.9, 4.5), method=\"Brent\", bounds=[-1, 1],\n",
    "                    options={\"maxiter\": iter}, tol=np.finfo(1.).eps)\n",
    "        if result.success:\n",
    "            break\n",
    "\n",
    "        this_x = result.x\n",
    "        all_x.append(this_x)\n",
    "        all_y.append(f(this_x))\n",
    "        if iter < 6:\n",
    "            plt.text(this_x - .05*np.sign(this_x) - .05,\n",
    "                    f(this_x) + 1.2*(.3 - iter % 2), iter + 1,\n",
    "                    size=12)\n",
    "    \n",
    "    plt.title(f\"epsilon = {epsilon}, converged at iter = {iter}\")\n",
    "    plt.plot(all_x[:10], all_y[:10], 'k+', markersize=12, markeredgewidth=2)\n",
    "\n",
    "    plt.plot(all_x[-1], all_y[-1], 'rx', markersize=12)\n",
    "    plt.axis('off')\n",
    "    plt.ylim(ymin=-1, ymax=8)\n",
    "\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    plt.semilogy(np.abs(all_y - all_y[-1]), linewidth=2)\n",
    "    plt.ylabel('Error on f(x)')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c63eb06-e7a8-4686-af4e-116034c04f4a",
   "metadata": {},
   "source": [
    "**Brent’s method on a quadratic function**: it converges in 3 iterations, as the quadratic approximation is then exact.<br>\n",
    "**Brent’s method on a non-convex function**: note that the fact that the optimizer avoided the local minimum is a matter of luck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac8c2c-7de2-43ca-ab9b-eead9797bd07",
   "metadata": {},
   "source": [
    "Try to use different solver by changing the `method` value. Options: `{\"Brent\", \"Bounded\", \"Golden\"}`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a766c634-daab-4bd2-b2af-e01bb39244e0",
   "metadata": {},
   "source": [
    "## N-dimensions\n",
    "\n",
    "Let's use the Rosenbrock function as an N-D exemple:\n",
    "\n",
    "$$f(\\textbf{x})=\\sum^{N-1}_{i=1}100 (x_{i+1} - x_i^2)^2+(1-x_i)^2$$\n",
    "\n",
    "The minimum of this function is 0, which is achieved when all $x_i = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee592be-66fd-4888-a57a-b07c2f8c7eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen(x):\n",
    "    \"\"\"The Rosenbrock function\"\"\"\n",
    "    return sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221ce4c-8cfa-4298-984a-157c9fceb0d2",
   "metadata": {},
   "source": [
    "### Nelder-Mead Simplex algorithm (`method='Nelder-Mead'`)\n",
    "\n",
    "The simplex algorithm is a generalization of dichotomy approaches to high-dimensional spaces. It is probably the simplest way to minimize a fairly well-behaved function. It requires only function evaluations and is a good choice for simple minimization problems. \n",
    "\n",
    "<img src=\"../asset/nelder-mead-schema.png\" width=\"400\">\n",
    "\n",
    "[image credits](https://www.youtube.com/watch?v=-GWze-wtu60&ab_channel=BYUFLOWLab)\n",
    "\n",
    "A simplex in 2D in just a triangle. The point $x^{(0)}$ is considered the best point, $x^{(1)}$ the second best and $x^{(2)}$ the worst. We compute the centroid of our simplex $x_c$, excluding the worse point, and we reflect that worst point with respect to the centroid.\n",
    "\n",
    "\n",
    "**Strength**: it is robust to noise, as it does not rely on computing gradients. Thus it can work on functions that are not locally smooth such as experimental data points, as long as they display a large-scale bell-shape behavior. However it is slower than gradient-based methods on smooth, non-noisy functions.\n",
    "\n",
    "However, because it does not use any gradient evaluations, it may take longer to find the minimum.\n",
    "\n",
    "\n",
    "<img src=\"../asset/nelder-mead.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbdb46-1b90-46e0-b4dc-06cea13edf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='nelder-mead',\n",
    "    options={'xatol': 1e-8, 'disp': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a015c-3dac-44f3-aa83-24a01bb31d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579b7fe-9938-4aa0-842a-85601fc32516",
   "metadata": {},
   "source": [
    "Ususally, we expect the number of function evaluations to be greater than the number iterations, because a single iteration call at least one function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fad146-0dcc-4db2-94e1-d32284bb2953",
   "metadata": {},
   "source": [
    "### Powell algorithm `(method='Powell')`\n",
    "\n",
    "Another optimization algorithm that doesn't need gradients. It works by minimizing the function along 2 axis individually, and update these axis as such:\n",
    "\n",
    "<img src=\"../asset/powell-schema.png\" width=\"800\">\n",
    "\n",
    "[image credits](https://www.youtube.com/watch?v=1Z_4sBNoZj4&ab_channel=EMPossible)\n",
    "\n",
    "\n",
    "Use it with `method='powell'` in minimize.\n",
    "\n",
    "<img src=\"../asset/powells.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a18791-b4ed-4522-8370-aebf26e22e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='powell',\n",
    "    options={'xtol': 1e-8, 'disp': True},  # note that the tolerance parameter `xatol` is now called `xtol`\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea65593-7bd1-4885-907c-0a37947956c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368721c1-da68-4dc9-9152-8fff154357e9",
   "metadata": {},
   "source": [
    "**Question**: in this exemple, is Nelder-Mead's method faster than Powell's method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eedc245-940b-41c5-9b7f-eb86babf44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='nelder-mead',\n",
    "    options={'xtol': 1e-8, 'disp': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e00ed32-780c-4d0b-b359-605e85cfbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='powell',\n",
    "    options={'xtol': 1e-8, 'disp': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0e4aa-a972-4269-9915-a16a19fb56d1",
   "metadata": {},
   "source": [
    "We can also tweak our function to optimize and add some parameters, as in the following:\n",
    "\n",
    "$$f(\\textbf{x}, a, b)=\\sum^{N-1}_{i=1}a(x_{i+1} - x_i^2)^2+(1-x_i)^2+b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74f03e-cc33-4db8-a8d4-e24c948655fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen_params(x, a, b):\n",
    "    #TODO\n",
    "    return sum(a * (x[1:] - x[:-1]**2) ** 2 + (1 - x[:-1]) ** 2 + b)\n",
    "\n",
    "res = optimize.minimize(\n",
    "    rosen_params,\n",
    "    x0,\n",
    "    method='nelder-mead',\n",
    "    options={'xatol': 1e-8, 'disp': True},\n",
    "    args=(0.5, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98fd60-272e-48bc-b98d-a4bd76cb2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca4c134-936f-44a9-9b41-9a180988f0a0",
   "metadata": {},
   "source": [
    "### Gradient Based methods\n",
    "\n",
    "***Here we focus on intuitions, not code. Code will follow.***\n",
    "\n",
    "\n",
    "Gradient descent basically consists in taking small steps in the direction of the gradient, that is the direction of the steepest descent.\n",
    "\n",
    "$x_{k+1} = x_k - \\alpha \\nabla f(x_k) $\n",
    "\n",
    "<img src=\"../asset/gradient-based.png\" width=\"800\">\n",
    "\n",
    "We can see that very anisotropic (ill-conditioned) functions are harder to optimize. If you know natural scaling for your variables, prescale them so that they behave similarly. This is related to preconditioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06fd63-58a8-401f-9e8a-bd0c56a4e946",
   "metadata": {},
   "source": [
    "### Adaptive set gradient descent\n",
    "\n",
    "It can be advantageous to take bigger steps. This is done by gradient descent using line search.\n",
    "\n",
    "Line search is finding the right step size for an iteration, with the condition:\n",
    "\n",
    "$f(x_k) - f(x_k + \\alpha_k d_k) \\geq \\alpha_k \\gamma$, with $\\gamma > 0$ and $d_k$ the chosen direction.\n",
    "\n",
    "<img src=\"../asset/adap.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bba3b39-4c6a-44bc-aaaf-83f94998a3e5",
   "metadata": {},
   "source": [
    "### Conjugate gradient descent\n",
    "\n",
    "The gradient descent algorithms above are toys not to be used on real problems.\n",
    "\n",
    "One of the problems of the simple gradient descent algorithms, is that it tends to oscillate across a valley, each time following the direction of the gradient, that makes it cross the valley. \n",
    "\n",
    "The conjugate gradient solves this problem by adding a friction term: each step depends on the two last values of the gradient and sharp turns are reduced.\n",
    "\n",
    "<img src=\"../asset/cg-g.png\" width=\"800\">\n",
    "\n",
    "Let's now code a bit.\n",
    "\n",
    "The gradient of the Rosenbrock function is the following:\n",
    "\n",
    "$$f(\\textbf{x})=\\sum^{N-1}_{i=1}(x_{i+1} - x_i^2)^2+(1-x_i)^2$$\n",
    "$$\\frac{\\partial f}{\\partial x_j} = \\sum_{i=1}^N 200 (x_{i} - x_{i-1}^2) (\\delta_{i, j} - 2 x_{i-1} \\delta_{i-1, j}) - 2 (1 - x_{i-1}) \\delta_{i-1, j} \\\\ = 200 (x_j - x_{j-1}^2) - 400 x_j (x_{j+1} - x_j ^ 2) - 2(1 - x_{j})$$\n",
    "\n",
    "This expression is valid for the edges as well:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_0} = - 400 x_0 (x_1 - x_0 ^ 2) - 2 (1 - x_0)$$\n",
    "$$\\frac{\\partial f}{\\partial x_{N-1}} = 200 (x_{N-1} - x^2_{N-2})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3533487-592d-476e-a6c4-9976831297e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen_der(x):\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return der"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba620fde-f985-4a8d-832a-91e462c0e4b6",
   "metadata": {},
   "source": [
    "This gradient information is specified in the minimize function through the `jac` parameter as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa11a085-c528-458a-b758-f16037d4bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    jac=rosen_der,\n",
    "    method='CG',\n",
    "    options={'disp': True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e59827-8b2e-440d-82c4-e2bca10ed98b",
   "metadata": {},
   "source": [
    "In order to converge more quickly to the solution, this routine uses the gradient of the objective function. If the gradient is not given by the user, then it is estimated using first-differences, but will perform better if you can pass them the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a483bf3-7eab-42c4-8ba9-c0e833106405",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='CG',\n",
    "    options={'disp': False},\n",
    "    jac=rosen_der,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff15e07-6024-4e12-a4fd-77082e1b4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='CG',\n",
    "    options={'disp': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b273ff-2a32-47ba-bbfc-74af61d49dc3",
   "metadata": {},
   "source": [
    "### Newton-Conjugate-Gradient algorithm `(method='Newton-CG')`\n",
    "\n",
    "Newton-Conjugate Gradient algorithm is a modified Newton’s method and uses a conjugate gradient algorithm to (approximately) invert the local Hessian. Newton’s method is based on fitting the function locally to a quadratic form:\n",
    "\n",
    "$$f(\\textbf{x})=f(\\textbf{x}_0) +  \\nabla f(\\textbf{x}_0) (\\textbf{x} - \\textbf{x}_0) + \\frac{1}{2} (\\textbf{x} - \\textbf{x}_0)^{\\top} \\textbf{H}(\\textbf{x}_0) (\\textbf{x} - \\textbf{x}_0) $$\n",
    "\n",
    "where $\\textbf{H}$ is a matrix of second-derivatives (the Hessian). If the Hessian is positive definite then the local minimum of this function can be found by setting the gradient of the quadratic form to zero, resulting in:\n",
    "\n",
    "$$\\textbf{x}_{opt} = \\textbf{x}_0 - \\textbf{H}^{-1}\\nabla f$$\n",
    "\n",
    "Due to the Hessian inversion, the complexity is $O(N^3)$\n",
    "\n",
    "<img src=\"../asset/newton-cg.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e658db-243f-44d1-814e-9bc58b8b26ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen_hess(x):\n",
    "    x = np.asarray(x)\n",
    "    H = np.diag(-400*x[:-1],1) - np.diag(400*x[:-1],-1)\n",
    "    diagonal = np.zeros_like(x)\n",
    "    diagonal[0] = 1200*x[0]**2-400*x[1]+2\n",
    "    diagonal[-1] = 200\n",
    "    diagonal[1:-1] = 202 + 1200*x[1:-1]**2 - 400*x[2:]\n",
    "    H = H + np.diag(diagonal)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc8f1c6-3ccb-4599-baca-ed74e2883e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize.minimize(rosen_obj_and_der, x0, method='Newton-CG', jac=True, hess=rosen_hess,\n",
    "               options={'disp': False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad785cf8-b784-484f-93a3-fd4ff48099c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dc8b9-3f3d-4499-b692-6b41995d510f",
   "metadata": {},
   "source": [
    "The Hessian can also be computed numerically, but the method will also be faster if you provide the hessian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6802146-b536-43f2-ad4f-ce2d6cd59917",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='Newton-CG',\n",
    "    jac=rosen_der,\n",
    "    hess=rosen_hess,\n",
    "    options={'disp': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed890f-fe63-4bad-bfb3-3223f23dd5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='Newton-CG',\n",
    "    jac=rosen_der,\n",
    "    options={'disp': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4797e1a-b9ed-491d-ad3e-b176cdeba4d4",
   "metadata": {},
   "source": [
    "### Quasi-Newton: BFGS algorithm `(method='BFGS')`\n",
    "\n",
    "Refine an approximation of the Hessian at each steps, introducing a complexity in $O(n^2)$.\n",
    "\n",
    "The BFGS method typically requires fewer function calls than the simplex algorithm even when the gradient must be estimated.\n",
    "\n",
    "In very high dimensions (> 250) the Hessian matrix is too costly to compute and invert.  Limited-memory BFGS (L-BFGS) sits between BFGS and conjugate gradient keeps a low-rank version. In addition, box bounds are also supported by L-BFGS-B:\n",
    "\n",
    "<img src=\"../asset/bfgs.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65e9da-45c0-4767-b1e2-e376c4078836",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='BFGS',\n",
    "    jac=rosen_der,\n",
    "    options={'disp': True}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e6fb7-591e-4ed7-bb6e-cf6fbd892a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3ddd5-c636-4409-8c76-1949d1fcdbeb",
   "metadata": {},
   "source": [
    "On possible speed-up is to unite the objective function and its gradient in a single function. This helps avoiding redundant computation.\n",
    "\n",
    "This is implemented by setting `jac=True` instead of the `der` function and passing a tuple of `(objective, der)` functions as first input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c3a756-2087-4ee5-8deb-351c4c351ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen_obj_and_der(x):\n",
    "    objective = sum(100.0*(x[1:]-x[:-1]**2.0)**2.0 + (1-x[:-1])**2.0)\n",
    "    xm = x[1:-1]\n",
    "    xm_m1 = x[:-2]\n",
    "    xm_p1 = x[2:]\n",
    "    der = np.zeros_like(x)\n",
    "    der[1:-1] = 200*(xm-xm_m1**2) - 400*(xm_p1 - xm**2)*xm - 2*(1-xm)\n",
    "    der[0] = -400*x[0]*(x[1]-x[0]**2) - 2*(1-x[0])\n",
    "    der[-1] = 200*(x[-1]-x[-2]**2)\n",
    "    return objective, der"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0b4f2-240b-4ebd-b4bf-8da4abef570c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "res = optimize.minimize(\n",
    "    rosen,\n",
    "    x0,\n",
    "    method='BFGS',\n",
    "    jac=rosen_der,\n",
    "    options={'disp': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4358b9ba-8b94-42e5-b90f-4d1e0e257d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "_ = optimize.minimize(\n",
    "    rosen_obj_and_der,\n",
    "    x0,\n",
    "    method='BFGS',\n",
    "    jac=True,\n",
    "    options={'disp': False}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63c2f0-ad63-4338-83b7-09689f98bf7e",
   "metadata": {},
   "source": [
    "## In a nutshell\n",
    "\n",
    "- **Without knowledge of the gradient:**\n",
    "    - Prefer BFGS or L-BFGS, even if you have to approximate numerically gradients. They are the default method of `minimize`.\n",
    "    - On well-condionned problem, Powell and Nelder-Mead, both gradient-free methods, work well in high-dimension, but collapse on ill-conditioned problem.\n",
    "- **With knowledge of the gradient:**\n",
    "    - BFGS and L-BFGS\n",
    "    - Conjugate gradient is more effective for computing computationally cheap functions.\n",
    "- **With the Hessian:**\n",
    "    - Prefer the Newton-CG method\n",
    "- **If you have noisy measurements:**\n",
    "    - Powell and Nelder-Mead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ac825-a1bf-456b-893d-b3edec7cae08",
   "metadata": {},
   "source": [
    "### Exercice 1: A simple (?) quadratic function\n",
    "\n",
    "Optimize the following function, using K[0] as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd970218-4d7e-4a70-99f3-9ff064b7b784",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "K = np.random.normal(size=(100, 100))\n",
    "\n",
    "def f(x):\n",
    "    return np.sum((np.dot(K, x - 1))**2) + np.sum(x**2)**2\n",
    "\n",
    "def f_der(x):\n",
    "    return 2*np.dot(np.dot(K.T, K), x - 1) + 4*np.sum(x**2)*x\n",
    "\n",
    "def f_hessian(x):\n",
    "    H = 2*np.dot(K.T, K) + 4*2*x*x[:, np.newaxis]\n",
    "    return H + 4*np.eye(H.shape[0])*np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189138e-a0b8-4362-a89b-646849a07bf1",
   "metadata": {},
   "source": [
    "Hint: use the `evaluate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef9e0f-ccdb-49c0-8727-829f1067286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(method_name, x_ref, x_some_method, compute_time):\n",
    "    print(\n",
    "        f\"{method_name} -- x_squared error: {sum((x_ref - x_some_method)** 2):.6f}, \"\n",
    "        f\"y diff: {f(x_some_method) - f(x_ref):.6f}, \"\n",
    "        f\"time: {compute_time:.4f}s\"\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7636b-7d26-4c65-b455-00618ba0cc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "x_ref = optimize.minimize(f, K[0], method=\"Powell\").x\n",
    "end = time()\n",
    "\n",
    "evaluate(\"Powell\", x_ref, x_ref, end - start)\n",
    "\n",
    "### TODO - Write your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701e4bb7-bea6-4b7b-9975-288f40363517",
   "metadata": {},
   "source": [
    "Time your approach. Find the fastest approach. Why is BFGS not working well?\n",
    "\n",
    "[Solution](https://scipy-lectures.org/advanced/mathematical_optimization/auto_examples/plot_exercise_ill_conditioned.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e142bf-e3ad-4170-a7f7-95d2863c70b4",
   "metadata": {},
   "source": [
    "### Exercice 2: Finding a minimum in a flat neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d943dc-2a98-4d72-b2ea-775979e381d2",
   "metadata": {},
   "source": [
    "Consider the function: $\\exp(\\frac{-1}{0.1x^2 + y^2})$. It admits a minimum in (0, 0). Starting from a init at (1, 1), try to get within 1e-8 of this minimum point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec84ef2-8b34-482f-aca2-2a73dfc155c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.exp(-1/(.1 * x[0]**2 + x[1]**2))\n",
    "\n",
    "### TODO - Write your code here ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35664875-fe62-4c09-973a-638b18fe23dd",
   "metadata": {},
   "source": [
    "[Solution](https://scipy-lectures.org/advanced/mathematical_optimization/auto_examples/plot_exercise_flat_minimum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ed4fda-a22e-4881-974d-bd104b43b197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
