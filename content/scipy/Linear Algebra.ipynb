{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db8bf41",
   "metadata": {},
   "source": [
    "![euroscipy-logo](../asset/euroscipy2022-logo.png)\n",
    "\n",
    "# Linear Algebra with Scipy\n",
    "\n",
    "\n",
    "*Reading time ~45 min*\n",
    "\n",
    "- *Authored by*: [Vincent Maladiere](https://github.com/Vincent-Maladiere)\n",
    "- *Reviewed by*: [Julien Jerphanion](https://github.com/jjerphan), [Guillaume Lemaitre](https://github.com/glemaitre)\n",
    "- *Credits*: [Scipy linalg Tutorial](https://docs.scipy.org/doc/scipy/tutorial/linalg.html), [Scikit-Learn Tutorial](https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html#sphx-glr-auto-examples-neighbors-plot-nca-dim-reduction-py)\n",
    "\n",
    "\n",
    "`scipy.linalg` provide efficient linear algebra operation through BLAS and LAPACK backend. Input and ouput expect object that can be converted to 2-D arrays.\n",
    "\n",
    "### BLAS & LAPACK\n",
    "\n",
    "BLAS (Basic Linear Algebra Subroutine) is the standard specification for Linear Algebra. Many CPU-based distributions such as: OpenBLAS, ATLAS, MKL or BLIS, specialized for different hardwares. Also GPGPU distributions like cuBLAS. [BLAS functions](https://docs.scipy.org/doc/scipy/reference/linalg.blas.html) are categorized into 3 types of routines:\n",
    " \n",
    "**Level 1:** <br>\n",
    "Vector operations: $y \\leftarrow \\alpha x + y$, called `axpy` (\"a x plus y\"), also dot products and norms. Complexity in $O(N)$.\n",
    "\n",
    "\n",
    "**Level 2:** <br>\n",
    "Matrix - vector operations: $y \\leftarrow \\alpha A x + \\beta y$ through a generalized matrix-vector multiplication `gemv`. <br>\n",
    "Also solving the equation $Tx = y$ with $T$ being triangular. Complexity in $O(N^2)$.\n",
    "\n",
    "\n",
    "**Level 3:** <br>\n",
    "Matrix - Matrix operations: $C \\leftarrow \\alpha AB + \\beta C$ through generalized matrix multiplication `gemm`. <br>\n",
    "Used to inverse matrix as well. Complexity in $O(N^3)$.\n",
    "\n",
    "\n",
    "LAPACK (Linear Algebra Package) is another standard based on BLAS level 3. It implements linear equations solvers, linear least squares and eigen values decompositions.\n",
    "\n",
    "\n",
    "### Numpy linalg vs Scipy linalg\n",
    "\n",
    "> `scipy.linalg` contains all the functions in `numpy.linalg`. plus some other more advanced ones not contained in `numpy.linalg`.\n",
    "\n",
    "> Another advantage of using `scipy.linalg` over `numpy.linalg` is that it is always compiled with BLAS/LAPACK support, while for numpy this is optional. Therefore, the scipy version might be faster depending on how numpy was installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb641dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy import linalg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61e670f-2835-4e0b-938f-9086ed7a76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf37ee5",
   "metadata": {},
   "source": [
    "## Standard operations\n",
    "\n",
    "We will review some of the pillar methods of linear algebra for matrices:\n",
    "- The **determinant** is mostly used for determining if a matrix is singular and to compute the matrix inverse otherwise.\n",
    "- The **inverse** of a matrix is a very common operator in fields like machine learning and optimizations.\n",
    "- **Eigenvalues** and **eigenvectors** characterizes a matrix. Roughly speaking, the eigenvalues of a linear mapping is a measure of the distortion induced by the transformation and the eigenvectors tell you about how the distortion is oriented\n",
    "- **Singularvalues** of a matrix $X$ are the square root of the eigenvalues of $X^{\\top}X$, and are very useful to perform matrix decomposition (SVD)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0cb401",
   "metadata": {},
   "source": [
    "### Finding the determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6e5db6",
   "metadata": {},
   "source": [
    "The determinant of a square matrix $A$ is often denoted $|A|$ and is a quantity often used in linear algebra.\n",
    "\n",
    "2 computation techniques:\n",
    "\n",
    "**1. Leibniz formula**\n",
    "\n",
    "$$A=\\begin{bmatrix} a_{1,1} & a_{1,2} & a_{1,3} \\\\ a_{2,1} & a_{2,2} & a_{2,3} \\\\ a_{3,1} & a_{3,2} & a_{3, 3}\\end{bmatrix}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "| Permutation $\\sigma$ | ${sgn}(\\sigma )$ | $sgn(\\sigma) \\prod _{i=1}^{n}a_{i,\\sigma _{i}}$ |\n",
    "|--------------------------------------|-----------------------------------------------|--------------------------------------------------------------------------------|\n",
    "| 1, 2, 3                              | $+1$                           | $+a_{1,1}a_{2,2}a_{3,3}$                                        |\n",
    "| 1, 3, 2                              | $-1$                           | $-a_{1,1}a_{2,3}a_{3,2}$                                        |\n",
    "| 3, 1, 2                              | $+1$                           | $+a_{1,3}a_{2,1}a_{3,2}$                                        |\n",
    "| 3, 2, 1                              | $-1$                           | $-a_{1,3}a_{2,2}a_{3,1}$                                        |\n",
    "| 2, 3, 1                              | $+1$                           | $+a_{1,2}a_{2,3}a_{3,1}$                                        |\n",
    "| 2, 1, 3                              | $-1$                           | ${-a_{1,2}a_{2,1}a_{3,3}}$                                        |\n",
    "\n",
    "\n",
    "The sign of $\\sigma$  is defined to be $+1$ whenever the reordering given by $\\sigma$ can be achieved by successively interchanging two entries an even number of times, and $-1$ whenever it can be achieved by an odd number of such interchanges.\n",
    "\n",
    "\n",
    "$$\\det(A) = \\sum_{\\sigma \\in S_n} \\Big( sgn(\\sigma) \\prod^n_{i=1} a_{i,\\sigma_i} \\Big) = a_{1,1}a_{2,2}a_{3,3} - a_{1,1}a_{2,3}a_{3,2} + a_{1,3}a_{2,1}a_{3,2} - a_{1,3}a_{2,2}a_{3,1} + a_{1,2}a_{2,3}a_{3,1} - a_{1,2}a_{2,1}a_{3,3}$$\n",
    "\n",
    "**2. Laplace expansion**\n",
    "\n",
    "![laplace-expansion](../asset/laplaceexpansion.png)\n",
    "\n",
    "Suppose  $a_{ij}$ are the elements of the matrix $A$ and let $M_{ij}=|A_{ij}|$ be the determinant of the matrix left by removing the $i$ row and $j$ column from $A$. Then, for any column $j$:\n",
    "\n",
    "$$\\det(A)=\\sum_{i=1}^N (-1)^{i+j} a_{ij} M_{ij}$$\n",
    "\n",
    "This is a recursive way to define the determinant, where the base case is defined by accepting that the determinant of a $1 \\times 1$ matrix is the only matrix element.\n",
    "\n",
    "In Scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4c2c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 3, 5], [2, 5, 1], [2, 3, 8]])\n",
    "print(A, \"\\n\")\n",
    "print(\"det:\", linalg.det(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c83c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_singular = np.array([[1, 3, 5], [2, 6, 10], [2, 3, 8]])\n",
    "print(A_singular, \"\\n\")\n",
    "print(\"det:\", linalg.det(A_singular))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e487c6c",
   "metadata": {},
   "source": [
    "### Finding the inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf4e94b",
   "metadata": {},
   "source": [
    "The inverse of a matrix $A$ is the matrix $B$, such that $AB = I$, where $I$ is the identity matrix consisting of ones down the main diagonal. Usually, $B$ is denoted  $B=A^{-1}$.\n",
    "\n",
    "In SciPy, the matrix inverse of the NumPy array, A, is obtained using `linalg.inv(A)`, or using `A.I` if `A` is a Matrix. \n",
    "\n",
    "Under the hood, it performs [Gaussian elimination](https://en.wikipedia.org/wiki/Invertible_matrix#Gaussian_elimination), via [Lower-Upper decomposition](https://en.wikipedia.org/wiki/LU_decomposition).\n",
    "\n",
    "For example:\n",
    "\n",
    "$$A=\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 5 & 1 \\\\ 2 & 3 & 8 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a7648",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 3, 5], [2, 5, 1], [2, 3, 8]])\n",
    "A_inv = linalg.inv(A)\n",
    "print(A, \"\\n\")\n",
    "print(A_inv, \"\\n\")\n",
    "print(A.dot(A_inv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d73ec",
   "metadata": {},
   "source": [
    "Dot product alternatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b376f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.dot(A, A_inv), \"\\n\")\n",
    "print(A @ A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f6959",
   "metadata": {},
   "source": [
    "**TODO #1**: Try to perform inversion on `A_singular`. What result do you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - write your code below ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc336ae-0559-4aaf-8173-9017f197232b",
   "metadata": {},
   "source": [
    "In practice, we don't use the inverse but the pseudo inverse `pinv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c83c4d8-6dc5-4592-8475-c8b59686c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "linalg.pinv(A_singular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e965a9-15a2-4e0e-96bd-3b1ea18b6342",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_singular @ linalg.pinv(A_singular) @ A_singular"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6f48c",
   "metadata": {},
   "source": [
    "### Solving a linear system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be458c02",
   "metadata": {},
   "source": [
    "Suppose we have the following equations to solve:\n",
    "    \n",
    "$$x + 3y + 5z = 10 \\\\ 2x + 5y + z = 8 \\\\ 2x + 3y + 8z = 3$$\n",
    "\n",
    "We could turn this problem into matrix computation:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 5 & 1 \\\\ 2 & 3 & 8 \\end{bmatrix} \\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix}=\\begin{bmatrix}10\\\\8\\\\3\\end{bmatrix}$$\n",
    "\n",
    "Thus our problem is now:\n",
    "\n",
    "$$\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} = \\begin{bmatrix} 1 & 3 & 5 \\\\ 2 & 5 & 1 \\\\ 2 & 3 & 8 \\end{bmatrix}^{-1} \\begin{bmatrix}10\\\\8\\\\3\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a0681",
   "metadata": {},
   "source": [
    "**TODO #2**: Solve the problem using only `linalg.inv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3de04",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 3, 5], [2, 5, 1], [2, 3, 8]])\n",
    "b = np.array([[10], [8], [3]])\n",
    "\n",
    "### TODO - write your code below ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0783fbdc",
   "metadata": {},
   "source": [
    "However, it is better to use the `linalg.solve` command, which can be faster and more numerically stable. In this case, it, however, gives the same answer as shown in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745349e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = linalg.solve(A, b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d09d81",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb0a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ x - b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d96b10",
   "metadata": {},
   "source": [
    "### Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812137e4",
   "metadata": {},
   "source": [
    "A wide range of norm definitions are available using different parameters to the order argument of `linalg.norm`. This function takes a rank-1 (vectors) or a rank-2 (matrices) array and an optional order argument (default is 2). Based on these inputs, a vector or matrix norm of the requested order is computed.\n",
    "\n",
    "For a vector\n",
    "\n",
    "$$||x||=\\begin{cases} max|x_i| & ord=inf\\\\ min|x_i| & ord=-inf \\\\ \\big(\\sum_i |x_i|^{ord} \\big)^{1/ord} & ord < inf \\end{cases}$$\n",
    "\n",
    "For a matrix\n",
    "\n",
    "$$||A||=\\begin{cases} max_i \\sum_j |a_{ij}| & ord = inf \\\\ min_i \\sum_j |a_{ij}| & ord = -inf \\\\ max_j \\sum_i |a_{ij}| & ord = 1\\\\ min_j \\sum_i |a_{ij}| & ord = -1 \\\\ max\\, \\sigma_i & ord = 2 \\\\ max \\,\\sigma_j & ord = -2 \\\\ \\sqrt{\\mathrm{trace}(A^{\\top}A}) & ord = \\mathrm{\"fro\"}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eabd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe9b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b604cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linalg.norm(A[0], np.inf))\n",
    "print(linalg.norm(A[0], -np.inf))\n",
    "print(linalg.norm(A[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linalg.norm(A))  # frobenius \"fro\" is the default\n",
    "print(linalg.norm(A, np.inf))\n",
    "print(linalg.norm(A, 1))\n",
    "print(linalg.norm(A, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09777670",
   "metadata": {},
   "source": [
    "### Eigenvalues and engenvectors\n",
    "\n",
    "The eigenvalue-eigenvector problem is one of the most commonly employed linear algebra operations. In one popular form, the eigenvalue-eigenvector problem is to find for some square matrix $A$, scalars $\\lambda$ and corresponding vectors $v$, such that:\n",
    "\n",
    "$$A v = \\lambda v$$\n",
    "\n",
    "For an $N \\times N$ matrix, there are  (not necessarily distinct) eigenvalues — roots of the (characteristic) polynomial\n",
    "\n",
    "$$| A - \\lambda I | = 0$$\n",
    "\n",
    "Let's consider again the matrix:\n",
    "\n",
    "$$A=\\begin{bmatrix} 1 & 5 & 2 \\\\ 2 & 4 & 1 \\\\ 3 & 6 & 2 \\end{bmatrix}$$\n",
    "\n",
    "The characteristic polynomial is:\n",
    "\n",
    "$$|A - \\lambda I| = \\begin{bmatrix} 1-\\lambda & 5 & 2 \\\\ 2 & 4-\\lambda & 1 \\\\ 3 & 6 & 2-\\lambda \\end{bmatrix} = (1-\\lambda) [(4-\\lambda)(2-\\lambda)-6]-5[2(2-\\lambda)-3]+2[12-3(4-\\lambda)]\\\\=-\\lambda^3+7\\lambda^2+8\\lambda-3$$ \n",
    "\n",
    "The roots of this polynomial are:\n",
    "\n",
    "$$\\lambda_1 = 7.9579\\\\\\lambda_2 = -1.2577\\\\\\lambda_3=0.2997$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bf368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 5, 2], [2, 4, 1], [3, 6, 2]])\n",
    "la, v = linalg.eig(A)\n",
    "print(\"lambdas:\\n\", la, \"\\n\")\n",
    "print(\"right v:\\n\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b9129",
   "metadata": {},
   "source": [
    "Sanity check the initial equation $A v_i = \\lambda_i v_i $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61992fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(3):\n",
    "    print(linalg.norm(A @ v[:, idx] - la[idx] * v[:, idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875484b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c15f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "la * v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "A @ v - la * v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec0b9ac",
   "metadata": {},
   "source": [
    "**TODO #3**: Which operation `la * v` perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57f1d1",
   "metadata": {},
   "source": [
    "Eigenvectors are unitary (defining a orthonormal basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184adab",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa240e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(v**2).sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5693f66",
   "metadata": {},
   "source": [
    "### Singular value decomposition (SVD)\n",
    "\n",
    "Singular value decomposition (SVD) can be thought of as an extension of the eigenvalue problem to matrices that are not square. \n",
    "\n",
    "Let $A$ be a $N \\times M$ matrix. $A^{\\top} A$ and $A A ^{\\top}$ are squared hermitian matrix of dimension $M \\times M$ and $N \\times N$. They are real, positive-definite matrix and hence invertible. \n",
    "\n",
    "Let's call their eigenvalues $\\sigma_i^2$. The singular values of $A$ are then $\\sigma_i$.\n",
    "\n",
    "$A$ also admit the SVD decomposition:\n",
    "\n",
    "$$A = U \\Sigma V^{\\top} $$\n",
    "\n",
    "With \n",
    "- $\\Sigma = diag(\\sigma_1, \\sigma_2, ..., \\sigma_k)$\n",
    "- $k = min(M, N)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "n, m = A.shape\n",
    "u, s, v = linalg.svd(A)\n",
    "print(u, \"\\n\")\n",
    "print(s, \"\\n\")\n",
    "print(v, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbe985",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = linalg.diagsvd(s, n, m)\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b76600",
   "metadata": {},
   "source": [
    "**TODO #4** Check that the decomposition is correct by computing the Frobenius norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - Write your code below ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b552e4a8-6bfb-46eb-88e3-97d842643bd0",
   "metadata": {},
   "source": [
    "### Exercice 1: Dataset decomposition\n",
    "\n",
    "Let's now use eigenvalues to map features to lower input space. This means that we can shrink dimensions without loosing too much information, which is notably helpful for visualisation purposes.\n",
    "\n",
    "We use data from the digit dataset, consisting in gray-scale value of images of hand-written digits:\n",
    "\n",
    "<img src=\"../asset/hand-written-digits.png\" alt=\"drawing\" style=\"width:200px;\"/>\n",
    "\n",
    "For this task we will:\n",
    "1. Center the data: $X_{center} = \\{(X_j - \\mu_j)\\}_{j=1:m}$, i.e. compute the mean of each column and substract it: `X - X.mean(axis=0)`.\n",
    "2. Scale the data: $X_{scaled} = \\{(X_{center, j}/\\sigma ^2_j)\\}_{j=1:m}$, i.e. compute the standard deviation of each column and scale it: `X / X.std(axis=0)`. You will probably need to add a small epsilon to the denominator for numerical stability: `epsilon = 1e-16`\n",
    "3. Compute the covariance matrix: $X_{center}^{\\top} . X_{center}$: i.e. `X.T @ X`\n",
    "4. Compute the eigenvalues `la` and eigenvectors `v` of the covariance matrix.\n",
    "5. Transform your scaled data with a dot product of the eigenvectors: `X_trans = X_c @ u`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49247171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b2966c-1072-4fdb-9b20-dc0c0472740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2833da99-ed50-4b31-9546-c84fc8663494",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8655730f-f965-46d6-91e3-4bff64384cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - Write your code below ###\n",
    "\n",
    "\n",
    "### end of TODO ###\n",
    "\n",
    "px.scatter(\n",
    "    x=X_trans[:, 0],\n",
    "    y=X_trans[:, 1],\n",
    "    color=y.astype(str),\n",
    "    width=800,\n",
    "    height=400,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9448e-236e-4f34-aafe-0fbe9648ef54",
   "metadata": {},
   "source": [
    "In scikit-learn, PCA we don't compute the huge Gram Matrix $X^{\\top}.X$ Instead, we use a SVD from scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0546a-a936-4975-a6c0-c184c3a8caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "X_c = (X - X.mean(axis=0))/(X.std(axis=0)+1e-16)\n",
    "\n",
    "u, s, v = linalg.svd(X_c, full_matrices=False)\n",
    "X_trans = u * s\n",
    "\n",
    "px.scatter(\n",
    "    x=-X_trans[:, 0],\n",
    "    y=-X_trans[:, 1],\n",
    "    color=y.astype(str),\n",
    "    width=800,\n",
    "    height=400,\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b653cde-83b6-4832-9455-593d6b01929d",
   "metadata": {},
   "source": [
    "### Exercice 2: Implement your own PCA!\n",
    "\n",
    "> PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, PCA is implemented as a transformer object that learns  components in its `fit` method, and can be used on new data to project it on these components [with the `transform` method].\n",
    "[scikit learn doc](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
    "\n",
    "PCA is widely used in Machine Learning to reduce the dimensionality of a dataset and simplify the task of the downstream model. By doing so, it can also filter out some noise and avoid overfitting.\n",
    "\n",
    "In what follows, your task is to implement a minimalistic PCA to reduce the dimensionality of a digit dataset. This dataset consists in gray-scale value of images of hand-written digits:\n",
    "\n",
    "We will compare your performances with those of Scikit-Learn PCA by computing the accuracy of a k-Nearest Neighbors classifier. \n",
    "\n",
    "\n",
    "Your PCA class consists in 2 methods:\n",
    "\n",
    "- `fit`:\n",
    "    - Compute and store the u, s, v values\n",
    "<br><br>\n",
    "\n",
    "\n",
    "- `transform`:\n",
    "    - Compute the dot product: $X_{trans}=X.v$. <br><br>\n",
    "    \n",
    "    Note that $X$ shape is $(N \\times M)$, $v$ shape is $(M \\times n_{components})$ and thus $X_{trans}$ shape is $(N \\times n_{components})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9668614-e108-4341-a1fd-1f9b7ddf9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA as OriginalPCA\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c4389-db31-4136-bfe4-e43a31e87f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_score(pca, X, y, title, switch_x=False):\n",
    "    \n",
    "    # Split into train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.5, stratify=y, random_state=0\n",
    "    )\n",
    "    \n",
    "    pca.fit(X_train, y_train)\n",
    "    \n",
    "    # Fit the classifier on the PCA embeddings\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn.fit(pca.transform(X_train), y_train)\n",
    "    \n",
    "    # Get the accuracy score from the classifier\n",
    "    acc_knn = knn.score(pca.transform(X_test), y_test)\n",
    "    \n",
    "    # Get embedding on the entire dataset for visualization purposes\n",
    "    X_embedded = pca.transform(X)\n",
    "    \n",
    "    px.scatter(\n",
    "        x=-X_embedded[:, 0] if switch_x else X_embedded[:, 0],\n",
    "        y=X_embedded[:, 1],\n",
    "        color=y.astype(str),\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title=f\"{title} - KNN Accuracy: {acc_knn:.5f}\"\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380c944-4586-45ec-9e0c-03214f9872ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self, n_components, random_state=None):\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        ### TODO - Write your code below ###\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        ### TODO - Write your code below ###\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f891a73-749f-487c-882f-5eb895560e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Digits dataset\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "pca = make_pipeline(StandardScaler(), PCA(n_components=40, random_state=0))\n",
    "pca_og = make_pipeline(StandardScaler(), OriginalPCA(n_components=40, random_state=0))\n",
    "\n",
    "show_score(pca, X, y, \"My PCA\")\n",
    "show_score(pca_og, X, y, \"Scikit Learn PCA\", switch_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7e1d6-03eb-4c34-a67f-59d4f24c7933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
